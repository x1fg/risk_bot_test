import faiss
import asyncio
import html
import requests
import logging

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.tools.retriever import create_retriever_tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import BaseSingleActionAgent, AgentExecutor
from langchain.schema import AgentAction, AgentFinish
from langchain.agents import BaseSingleActionAgent
from langchain.schema import AgentFinish
from pydantic import BaseModel
from transformers import AutoTokenizer

from prompts import prompts_dict


class CustomAgent(BaseSingleActionAgent, BaseModel):
    llm: object
    tools: list
    prompt_template: object

    @property
    def input_keys(self):
        return ["input"]

    @property
    def output_keys(self):
        return ["output"]

    def plan(self, intermediate_steps, **kwargs):
        input_data = kwargs["input"]
        prompt = self.prompt_template.format(input=input_data)
        response = self.llm(prompt)
        return AgentFinish({"output": response}, log="")

    async def aplan(self, intermediate_steps, **kwargs):
        return self.plan(intermediate_steps, **kwargs)


class DealAnalyzer:
    def __init__(self, llm, embeddings, docs_naming, tokenizer_model):
        self.llm = llm
        self.embeddings = embeddings
        self.docs_naming = docs_naming
        self.docs = []
        self.splitted_docs = []
        self.vector_store = None
        self.retriever_tool = None
        self.tools = []
        self.agents_dict = {}
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)

        self.risk_to_agent_map = {
            "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π": "financial_expert",
            "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–π": "marketing_expert",
            "—Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π": "reputation_expert",
            "–ø—Ä–∞–≤–æ–≤–æ–π": "lawyer",
        }

        self.load_documents()
        self.prepare_vector_store()
        self.create_tools()
        self.create_agents()

    def load_documents(self):
        for txt_file_path in self.docs_naming:
            try:
                loader = TextLoader(txt_file_path, encoding="utf-8")
                file_docs = loader.load()
                for doc in file_docs:
                    doc.metadata.update({'source_type': self.docs_naming[txt_file_path]})
                self.docs.extend(file_docs)
            except FileNotFoundError:
                print(f"File not found: {txt_file_path}")
            except UnicodeDecodeError:
                print(f"Encoding issue with file: {txt_file_path}")
            except Exception as e:
                print(f"Error loading file {txt_file_path}: {e}")

    def prepare_vector_store(self):
        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
            self.tokenizer, chunk_size=512, chunk_overlap=128)
        self.splitted_docs = text_splitter.split_documents(self.docs)
        index = faiss.IndexFlatL2(len(self.embeddings.embed_query("hello world")))
        self.vector_store = FAISS(
            embedding_function=self.embeddings,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
        )
        self.vector_store.add_documents(documents=self.splitted_docs)

    def create_tools(self):
        retriever = self.vector_store.as_retriever(search_type="mmr", search_kwargs={"k": 3})
        self.retriever_tool = create_retriever_tool(
            retriever,
            "deal_docs_search",
            "–°–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ –∫–ª–∏–µ–Ω—Ç–µ: –∑–∞–∫–ª—é—á–µ–Ω–∏—è —Å–ª—É–∂–±, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∏–µ–Ω—Ç–µ, —É—Å–ª–æ–≤–∏—è—Ö –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è."
        )
        search_tool = TavilySearchResults()
        self.tools = [self.retriever_tool, search_tool]

    def create_agents(self):
        agents_names = ['marketing_expert', 'lawyer', 'reputation_expert', 'financial_expert']
        for agent_name in agents_names:
            agent_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", prompts_dict[f'system_{agent_name}']),
                    ("human", "–í–æ–ø—Ä–æ—Å: {input}"),
                    ("placeholder", "{agent_scratchpad}"),
                ]
            )
            agent = CustomAgent(llm=self.llm, tools=self.tools, prompt_template=agent_prompt)
            agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=False)
            self.agents_dict[agent_name] = agent_executor

    async def generate_report(self, selected_risks, company_name, deal_details, message):
        results = {}

        for risk in selected_risks:
            agent_name = self.risk_to_agent_map.get(risk)
            if agent_name:
                try:
                    api_response = await self._call_api(risk, message)
                    api_info = self._extract_api_info(risk, api_response) if api_response else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ API"

                    deal_prompt = f"""
                    –ö–æ–º–ø–∞–Ω–∏—è: {company_name}
                    –î–µ—Ç–∞–ª–∏ —Å–¥–µ–ª–∫–∏: {deal_details}
                    –î–∞–Ω–Ω—ã–µ –∏–∑ API –ø–æ {risk} —Ä–∏—Å–∫—É: {api_info}
                    –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–∞: {risk}
                    """
                    response = self.agents_dict[agent_name].invoke({"input": deal_prompt})
                    result_text = response.get("output", "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞.")
                    result_text = clean_formatting(result_text)
                    results[risk] = html.escape(result_text)

                except Exception as e:
                    results[risk] = html.escape(f"–û—à–∏–±–∫–∞: {str(e)}")

        report_lines = [f"üí° <b>–°–¥–µ–ª–∫–∞:</b> {html.escape(clean_formatting(company_name))}\n"]
        report_lines.append(f"<b>–î–µ—Ç–∞–ª–∏ —Å–¥–µ–ª–∫–∏:</b> {html.escape(clean_formatting(deal_details))}\n")

        for risk, result in results.items():
            report_lines.append(f"<b>{html.escape(clean_formatting(risk.capitalize()))} —Ä–∏—Å–∫</b>")
            report_lines.append(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\n{result}\n")

        risk_table = self.generate_risk_table(results)
        report_text = "\n".join(report_lines)
        full_report = f"{report_text}\n<b>–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–∏—Å–∫–æ–≤</b>:\n<pre>{html.escape(risk_table)}</pre>"

        report_chunks = split_message(full_report)
        for chunk in report_chunks:
            await message.answer(chunk, parse_mode="HTML")

    async def _call_api(self, risk, message):
        api_urls = {
            "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π": 'http://83.220.174.239:9797/rating',
            "—Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π": 'http://83.220.174.239:9797/news',
            "–ø—Ä–∞–≤–æ–≤–æ–π": 'http://83.220.174.239:9797/juridical'
        }
        try:
            url = api_urls.get(risk)
            if url:
                await message.answer(f"üõ†Ô∏è –í—ã–∑–æ–≤ API –¥–ª—è —Ä–∏—Å–∫–∞: {risk}") 
                response = requests.get(url, timeout=10)
                return response.json()
        except requests.exceptions.RequestException as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ API –¥–ª—è —Ä–∏—Å–∫–∞ '{risk}': {str(e)}")
            await message.answer(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ API –¥–ª—è —Ä–∏—Å–∫–∞ '{risk}': {str(e)}")
            return None
        
    def _extract_api_info(self, risk, api_response):
        if risk == "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π":
            value = api_response.get('value', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö')
            return f"–û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Ä–∏—Å–∫–∞: {value}/20"
        elif risk in ["—Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π", "–ø—Ä–∞–≤–æ–≤–æ–π"]:
            return api_response.get('message', '–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π.')
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç API"

    def generate_risk_table(self, risk_results):
        table_header = "–†–∏—Å–∫              | –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞"
        table_separator = "-" * len(table_header)
        table_rows = []

        for risk, result in risk_results.items():
            risk_level = "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
            if "–Ω–∏–∑–∫–∏–π" in result.lower():
                risk_level = "–ù–∏–∑–∫–∏–π"
            elif "—Å—Ä–µ–¥–Ω–∏–π" in result.lower():
                risk_level = "–°—Ä–µ–¥–Ω–∏–π"
            elif "–≤—ã—Å–æ–∫–∏–π" in result.lower():
                risk_level = "–í—ã—Å–æ–∫–∏–π"

            table_rows.append(f"{risk.capitalize():<17} | {risk_level}")

        return "\n".join([table_header, table_separator, *table_rows])

def split_message(message, chunk_size=4000):
    if not isinstance(message, str):
        raise TypeError(f"split_message –æ–∂–∏–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É, –∞ –Ω–µ –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞ {type(message)}")

    chunks = []
    while len(message) > chunk_size:
        split_index = message.rfind("\n", 0, chunk_size)
        if split_index == -1:
            split_index = chunk_size
        chunks.append(message[:split_index])
        message = message[split_index:].strip()
    chunks.append(message)
    return chunks

def clean_formatting(text):
    text = text.replace("**", "")
    text = text.replace("####", "")
    text = text.replace("###", "")
    return text.strip()